# -*- coding: utf-8 -*-
"""Credit Loan Risk _ IDX Partners

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QyoBUjjRhukU_k64lcns-IkRYiEBnxEm

#Import Library

Pada tahapan ini, kita perlu melakukan import untuk library-library yang akan diperlukan dalam proses pemodelan dan analisis data.
"""

import os

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from scipy.stats import chi2_contingency

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

pd.set_option("display.max_columns", 50)
pd.set_option("display.max_rows", 100)
pd.set_option("display.max_colwidth", 1000)

"""#Data Wrangling

## Import Dataset

Definisikan nama file csv dan file feather yang akan disimpan.
"""

csv_filename = '/content/drive/MyDrive/Rakamin Academy/ID X Partners/loan_data_2007_2014.csv'
feather_filename = "loan_data_2007_2014.feather"

"""Import csv dan copy file menjadi file feather untuk kemudian digunakan seterusnya dalam program ini."""

from google.colab import drive
drive.mount('/content/drive')

if not os.path.exists(feather_filename):
    # Baca melalui csv
    df = pd.read_csv(csv_filename)

    # Ubah ke feather
    df.to_feather("loan_data_2007_2014.feather")

"""Convert file csv menjadi file feather untuk mempersingkat running dan load time dataset yang akan digunakan sepanjang pengolahan data."""

raw_df = pd.read_feather(feather_filename)

raw_df.head()

"""Import features/atribut dari dataset dan pilih sheet Excel yang digunakan, yaitu "LoanStats"
"""

data_dict = pd.read_excel("/content/drive/MyDrive/Rakamin Academy/ID X Partners/LCDataDictionary.xlsx", sheet_name="LoanStats")

"""Tampilkan features dataset"""

data_dict[["Feature","Description"]]

"""## Data Cleaning

During this stage, the data cleaning process is implemented to **address missing values** in specific columns. This involves identifying and handling gaps or null entries in the dataset, ensuring a more accurate and complete dataset for subsequent analyses.

Techniques such as imputation or removal of incomplete records is employed to enhance the overall data quality.
"""

raw_df.info()

"""Terlihat bahwa banyak kolom yang terdapat _missing values_ di dalamnya. Selanjutnya, kita urutkan persentase _missing values_ tiap kolom mulai dari terbesar hingga terkecil."""

(raw_df.isna().mean()*100).sort_values(ascending=False).head(30)

"""Next, the columns with >40% or nearly half missing values will be removed to make data analysis process easier and more efficient."""

missing_values = raw_df.isna().mean()*100
col_missingvalues = missing_values[missing_values > 40].index
col_missingvalues

raw_df.drop(col_missingvalues, axis = 1, inplace = True)

# Dropping/removing duplicate data in the dataset
raw_df.drop_duplicates(inplace=True)

print('id',raw_df['id'].nunique())
print('member id',raw_df['member_id'].nunique())

"""There is no more duplicate data in this dataset."""

raw_df.head(10)

"""### Handling Missing Value with Data Imputation"""

# Addressing missing value

## Categorical Columns
cat_col = ['emp_length', 'emp_title', 'title']
raw_df[cat_col] = raw_df[cat_col].fillna(value={'emp_length': '10+ years', 'emp_title': 'Teacher', 'title': 'Debt Consolidation'})

## Mapping term values to numeric
raw_df['term'] = raw_df['term'].replace({'36 months': 36, '60 months': 60})

## Numerical Columns
num_col = ['annual_inc', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_util', 'total_acc',
           'collections_12_mths_ex_med', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']
raw_df[num_col] = raw_df[num_col].fillna(value=raw_df[num_col].median())

## Datetime Columns
date_col = ['earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d']
raw_df[date_col] = raw_df[date_col].ffill()

raw_df.info()

"""### Datetype Data Converting

1. issue_d: The month which the loan was funded.
2. earliest_cr_line:The month the borrower's earliest reported credit line was opened
3. last_pymnt_d: Last month payment was received
4. last_credit_pull_d: The recent date a lender checked an individual's credit for a lending decision.
"""

datetype_df = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d']

for col in datetype_df:
  # Convert column to datetime format and adjust the year interpretation
  raw_df[col] = pd.to_datetime(raw_df[col], format='%b-%y')
  current_year = pd.Timestamp.now().year
  raw_df[col] = raw_df[col].apply(lambda x: x - pd.DateOffset(years=100) if x.year > current_year else x)

  # Calculate the months since the earliest credit line
  raw_df['months_since_'+col] = round(pd.to_numeric((pd.to_datetime('2023-11-01') - raw_df[col]) / np.timedelta64(1, 'M')))

  # Checking negative values in months
  if any(raw_df['months_since_'+col]<0) == False:
    print("Succeed")
  else:
    print("There are still negative values in the column")

  # Delete prior variable
  raw_df.drop(col, axis=1, inplace=True)

"""### Categorizing Numerical Variable"""

# Create a function to classify credit utilization rate
def classify_credit_utilization_rate(utilization_rate):
    if utilization_rate <= 10:
        return 'Excellent'
    elif 10 < utilization_rate <= 20:
        return 'Good'
    elif 20 < utilization_rate <= 30:
        return 'Fair'
    elif 30 < utilization_rate <= 50:
        return 'High'
    else:
        return 'Poor'

# Apply the classification function to create the new column
raw_df['credit_util_rate'] = raw_df['revol_util'].apply(classify_credit_utilization_rate)

"""## Label Checking"""

# Specify the list of categorical columns
categorical_cols = raw_df.select_dtypes(exclude='number').columns.tolist()
categorical_cols

for i in categorical_cols:
  df = raw_df[i]
  print('nunique variabel {} sebanyak {}'.format(i, df.nunique()))

# Drop emp_title, title, dan zip_code since it has a lot of categories
raw_df.drop(['emp_title','title','zip_code','url','application_type'], axis=1, inplace=True)

categorical_cols = [col for col in categorical_cols if col not in ['loan_status', 'emp_title', 'title', 'zip_code','url','application_type']]

# Extract numerical columns
numerical_cols = raw_df.select_dtypes(include='number').columns.tolist()
numerical_cols

for i in numerical_cols:
  df = raw_df[i]
  print('nunique variabel {} sebanyak {}'.format(i, df.nunique()))

# Drop Unnamed: 0, id, and member_id since it contains unique values and won't be used in analysis
raw_df.drop(['Unnamed: 0','id','member_id'], axis=1, inplace=True)

numerical_cols = [col for col in numerical_cols if col not in ['Unnamed: 0','id','member_id']]

"""## Loan Status Categorizing

Before we dive into the detailed exploration, let's first figure out which borrowers or data can be considered as either `'bad loans' or 'good loans'`. We'll do this by looking at the different categories in loan_status and sorting them accordingly.
"""

def count(df, y, **sns_kwargs):
    value_counts = df[y].value_counts()
    percentage = value_counts / value_counts.sum()
    percentage = percentage.apply("{:.2%}".format)
    return percentage

count(raw_df,'loan_status')

"""Based on the loan status categories provided, loans are commonly categorized as either "good" or "bad" based on their repayment status.

**Good Loans:**
1. Current
2. Fully Paid
3. Does not meet the credit policy. Status:Fully Paid

**Bad Loans:**
1. Charged Off
2. Late (31-120 days)
3. In Grace Period
4. Late (16-30 days)
5. Default
6. Does not meet the credit policy. Status:Charged Off

**Explanation**

***Good Loans:*** These categories generally indicate that the borrower is up to date on payments, has fully repaid the loan, or, in the case of "Does not meet the credit policy. Status:Fully Paid," the loan was fully paid despite not meeting the credit policy.

***Bad Loans:*** These categories suggest potential issues with loan repayment. "Charged Off" typically means the lender has given up on attempting to collect the debt. Late payments, in grace periods, and defaults indicate various levels of delinquency.
"""

Loan_Status = ['Charged Off', 'Default' , 'Does not meet the credit policy. Status:Charged Off','Late (31-120 days)', 'Late (16-30 days)', 'In Grace Period']
raw_df['Loan_Status'] = np.where(raw_df['loan_status'].isin(Loan_Status), "Bad", "Good")
raw_df['Loan_Status_num'] = np.where(raw_df['loan_status'].isin(Loan_Status), 1, 0)

count(raw_df,'Loan_Status')

"""# Data Exploration"""

# Set Seaborn theme
sns.set_theme(style="darkgrid")

title_style = dict(size=20, weight="bold")

def count_plot(df, y_col, main_title, **plot_args):
    counts = df[y_col].value_counts()
    percentages = counts / counts.sum()
    percentages_str = percentages.apply("{:.2%}".format)

    plt.figure(figsize=(14, 10))
    plt.title(main_title, fontdict=title_style)
    ax = sns.countplot(data=df, y=y_col, order=counts.index, **plot_args)
    plt.ylabel("")

    # Add percentage labels
    for i, p in enumerate(ax.patches):
        percentage_label = f"{percentages_str[i]}"
        ax.text(p.get_width() + 0.1, p.get_y() + p.get_height() / 2, percentage_label, ha='left', va='center')

    plt.show()


def distribution_plot(df, x_col, main_title, **plot_args):
    plt.figure(figsize=(14, 10))
    plt.title(main_title, fontdict=title_style)
    sns.histplot(data=df, x=x_col, kde=True, **plot_args)
    plt.ylabel("")
    plt.show()


def box_plot(df, x_col, y_col, main_title, **plot_args):
    plt.figure(figsize=(14, 10))
    plt.title(main_title, fontdict=title_style)
    sns.boxplot(data=df, x=x_col, y=y_col, **plot_args)
    plt.ylabel("")
    plt.show()

"""## Variables

### Loan Status

The majority of loans are in the "Current" status. In terms of success, loans that are successfully repaid have a higher ratio compared to those that are overdue.
"""

count_plot(raw_df, y_col='loan_status', main_title="Loan Status")

"""### Total Principal Received by Loan Status

The term "Principal" refers to the principal amount of the loan extended to the borrower. In other words, it is the original sum of money borrowed. Majority of the "Bad Loan" often struggle to repay the principal amount by the due date, as evident from the distribution of payments below. On average, the repayment amount is nearly approaching zero.
"""

distribution_plot(df=raw_df, x_col="total_rec_prncp", hue="Loan_Status", main_title="Total Principal Received by Loan Status")

"""### Charged-Off Recoveries by Loan Status

Recoveries from charged-off loans refer to the total amount that cannot be collected by the borrowing company because it has surpassed the due date. Consequently, the borrowing company can release the right to collect the debt by selling it to another company. From this, it's evident that individuals with poor loan statuses are the ones who predominantly have charged-off recoveries.
"""

distribution_plot(df=raw_df, x_col="recoveries", hue="Loan_Status", main_title="Charged-Off Recoveries by Loan Status")

"""### Loan Status by Grade

Most borrowers fall within the B, C, and D grades. However, those with a Bad Loan Status tend to have an average grade of C-D, highlighting a contrast with the Good Loan Status borrowers who, on average, hold a B-C grade.

"""

sns.countplot(raw_df, x='grade', hue='Loan_Status', order=['A','B','C','D','E','F','G'], palette='viridis')

"""### Purpose of Loan

Over 50% of borrowers obtained loans for debt consolidation, followed by purposes such as credit card repayment, home improvement, other unspecified reasons, and major purchases.
"""

count_plot(raw_df, y_col='purpose', main_title="Loan Purpose")

bad_raw_df = raw_df[raw_df['Loan_Status']=='Bad']

count_plot(bad_raw_df, y_col='purpose', main_title="Purpose by Bad Loan Status")

"""### State and Territory of Origin

These two graphs highlight the dominant states and territories of origin for borrowers in taking out loans. Overall, and specifically among those with a Bad Loan Status, California led with 15%, followed by New York.
"""

count_plot(raw_df, y_col='addr_state', main_title="States and Territory of Origin")

count_plot(bad_raw_df, y_col='addr_state', main_title="States and Territory of Origin by Bad Loan Status")

"""### Home Ownership


"""

count_plot(raw_df, y_col='home_ownership', main_title="Home Ownership")

count_plot(bad_raw_df, y_col='home_ownership', main_title="Home Ownership by Bad Loan Status")

"""### Earliest Credit Line by Loan Status

Recoveries from charged-off loans refer to the total amount that cannot be collected by the borrowing company because it has surpassed the due date. Consequently, the borrowing company can release the right to collect the debt by selling it to another company. From this, it's evident that individuals with poor loan statuses are the ones who predominantly have charged-off recoveries.
"""

distribution_plot(df=raw_df, x_col="months_since_earliest_cr_line", hue="Loan_Status", main_title="Earliest Credit Line Opened by Loan Status")

"""## Loan Status and Other Categorical Variables Association"""

cat = ['term', 'grade', 'emp_length', 'home_ownership', 'verification_status', 'purpose', 'addr_state', 'initial_list_status', 'credit_util_rate']
p_val = []

for col in cat:
  # Group by column and 'Loan_Status' and calculate the count
  grouped_df = raw_df.groupby([col,'Loan_Status']).size().reset_index(name='count')

  # Pivot the grouped DataFrame
  pivot_table = grouped_df.pivot(index=col, columns='Loan_Status', values='count').fillna(0)

  # Display the pivot table
  # print(pivot_table)

  # Create a contingency table
  contingency_table = pd.crosstab(raw_df['Loan_Status'], raw_df[col])

  # Perform the chi-square test
  chi2, p, _, _ = chi2_contingency(contingency_table)
  print(f"P-value for {col}: {p}")

  p_val += [p]

"""## Loan Status and Other Numerical Variables Correlation"""

# Calculate the correlation for each variables
correlations = (raw_df.select_dtypes(exclude=object)
                         .corr()
                         .dropna(how="all", axis=0)
                         .dropna(how="all", axis=1)
)

correlations['Loan_Status_num'].abs().sort_values(ascending=False)

# Filter correlation between vmin - vmax
vmin, vmax = 0.1, 0.99

unstack_corr = correlations.unstack()
pos_corr = (unstack_corr > vmin) & (unstack_corr < vmax)
neg_corr = (unstack_corr > -vmax) & (unstack_corr < -vmin)
high_corr = unstack_corr[pos_corr | neg_corr]

trimmed_corr = high_corr.sort_values(ascending=False).unstack()

# Make a mask for the lower triangular matrix
mask = np.zeros_like(trimmed_corr)
mask[np.triu_indices_from(mask)] = True

# Show the heatmap
plt.figure(figsize=(20, 20))
plot = sns.heatmap(
    trimmed_corr,
    annot=True,
    mask=mask,
    fmt=".2f",
    cmap="viridis",
    annot_kws={"size": 14})

plot.set_xticklabels(plot.get_xticklabels(), size=18)
plot.set_yticklabels(plot.get_yticklabels(), size=18)
plt.show()

affect_loan = high_corr.loc['Loan_Status_num'].abs().sort_values(ascending=False)
affect_loan

"""# Preprocessing Data"""

# Separate the dataset attribute and store it into a variable
X1 = pd.get_dummies(raw_df[cat])
X2 = raw_df[affect_loan.index.to_list()]
X = pd.concat([X1, X2], axis=1)

# Separate the dataset label and store it into a variable
y = raw_df['Loan_Status_num']

X.head()

X_col = list(X.columns.values)
print(X_col)

y.head()

"""## Scaling the dataset"""

from sklearn.preprocessing import StandardScaler

# Standardize Dataset Values
scaler = StandardScaler()
scaler.fit(X)
X = scaler.transform(X)

X = pd.DataFrame(X, columns=X_col)
X.head()

"""## Split the Dataset Into Train and Test Data"""

from sklearn.model_selection import train_test_split

# Split the training and testing data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=2106)

"""## Evaluating the Dataset's Imbalance"""

y_train.value_counts()

"""We can see that the response values are very imbalance. Since we have a large amount of data, which is more than >10.000 data, we can use undersampling technique to address the imbalance in datasets."""

!apt-get install python3.10

from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
from collections import Counter

under_sampler = RandomOverSampler(random_state=123)

X_resampled, y_resampled = under_sampler.fit_resample(X_train.values, y_train.ravel())
Counter(y_resampled)

"""Convert the array back to dataframe with Pandas."""

col = X_train.columns.to_list()

X_train = pd.DataFrame(X_resampled,
             columns=col)

y_train = pd.Series(y_resampled)

"""# Training Machine Learning Model"""

from sklearn.svm import SVC

SVClassifier = SVC()
SVClassifier.fit(X_train, y_train)
y_pred_SV = SVClassifier.predict(X_train)
y_pred_SV_test = SVClassifier.predict(X_test)

print(accuracy_score(y_train, y_pred_SV))
print(accuracy_score(y_test, y_pred_SV_test))

# Show the model accuracy
SVClassifier.score(X_test, y_test)